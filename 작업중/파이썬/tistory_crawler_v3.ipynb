{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 정보처리기사 코드 문제 크롤러 v3.4 (진짜 최종!)\n",
        "\n",
        "**🎯 문제 범위 정확히 추출!**\n",
        "\n",
        "## v3.4 수정\n",
        "- ✅ **문제 범위를 먼저 찾은 후 그 안의 코드만 추출**\n",
        "- ✅ 다른 문제의 코드가 섞이는 버그 수정\n",
        "- ✅ 각 문제마다 정확한 코드 매칭\n",
        "\n",
        "## 작동 원리\n",
        "1. HTML에서 \"12. 다음은\" 찾기\n",
        "2. \"13. 다음은\"까지의 범위 추출\n",
        "3. 그 범위 안의 colorscripter-code-table만 파싱\n",
        "4. 완벽!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 크롤러 클래스 정의 (문제 범위 정확히!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TistoryCrawler:\n    def __init__(self, category_url):\n        self.category_url = category_url\n        self.base_url = \"https://chobopark.tistory.com\"\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        }\n        self.code_problems = []\n        \n    def get_post_links(self):\n        \"\"\"카테고리 페이지에서 모든 포스트 링크 가져오기\"\"\"\n        print(\"포스트 링크를 수집 중...\")\n        \n        try:\n            response = requests.get(self.category_url, headers=self.headers)\n            response.encoding = 'utf-8'\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            post_links = []\n            \n            all_links = soup.find_all('a', href=True)\n            for link in all_links:\n                title = link.get_text(strip=True)\n                if '정보처리기사' in title and '실기' in title and '복원' in title:\n                    full_url = urljoin(self.base_url, link['href'])\n                    if full_url not in [p['url'] for p in post_links]:\n                        year_match = re.search(r'(\\d{4})년\\s*(\\d)회', title)\n                        if year_match:\n                            post_links.append({\n                                'url': full_url, \n                                'title': title,\n                                'year': year_match.group(1),\n                                'session': year_match.group(2)\n                            })\n            \n            post_links.sort(key=lambda x: (x['year'], x['session']), reverse=True)\n            \n            print(f\"총 {len(post_links)}개의 포스트를 찾았습니다.\")\n            return post_links\n            \n        except Exception as e:\n            print(f\"링크 수집 중 오류: {e}\")\n            return []\n    \n    def crawl_post_content(self, post_info):\n        \"\"\"개별 포스트 크롤링\"\"\"\n        print(f\"\\n크롤링 중: [{post_info['year']}년 {post_info['session']}회]\")\n        \n        try:\n            time.sleep(1)\n            response = requests.get(post_info['url'], headers=self.headers)\n            response.encoding = 'utf-8'\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            content_area = soup.find('div', class_='tt_article_useless_p_margin') or \\\n                          soup.find('article') or \\\n                          soup.find('div', {'class': 'content'})\n            \n            if not content_area:\n                content_area = soup.find('div', id='content') or soup.find('main')\n            \n            if content_area:\n                return {\n                    'text': content_area.get_text(separator='\\n', strip=True),\n                    'html': str(content_area),\n                    'year': post_info['year'],\n                    'session': post_info['session'],\n                    'url': post_info['url']\n                }\n            else:\n                print(f\"  ✗ 본문을 찾을 수 없습니다\")\n                return None\n                \n        except Exception as e:\n            print(f\"  ✗ 크롤링 오류: {e}\")\n            return None\n    \n    def extract_problem_content(self, text, html, problem_num):\n        \"\"\"특정 문제 번호의 전체 내용 추출\"\"\"\n        from bs4 import BeautifulSoup\n        \n        # HTML 파싱\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        # 전체 HTML을 문자열로\n        html_str = str(soup)\n        \n        # 문제 시작 위치 찾기 (HTML에서)\n        problem_pattern = rf'{problem_num}\\.\\s*다음은'\n        problem_match = re.search(problem_pattern, html_str)\n        \n        if not problem_match:\n            return None\n        \n        problem_start_pos = problem_match.start()\n        \n        # 다음 문제 찾기\n        next_num = int(problem_num) + 1\n        next_pattern = rf'{next_num}\\.\\s*다음은'\n        next_match = re.search(next_pattern, html_str[problem_start_pos + 10:])\n        \n        if next_match:\n            problem_end_pos = problem_start_pos + 10 + next_match.start()\n        else:\n            problem_end_pos = len(html_str)\n        \n        # 문제 범위의 HTML 추출\n        problem_html = html_str[problem_start_pos:problem_end_pos]\n        \n        # 이 범위의 HTML을 다시 파싱\n        problem_soup = BeautifulSoup(problem_html, 'html.parser')\n        \n        # 문제 텍스트 (정리된 버전)\n        problem_text = problem_soup.get_text().strip()\n        \n        # 코드 블록 추출 - Color Scripter 테이블만 찾기\n        code_blocks = []\n        \n        # colorscripter-code-table 찾기 (이제 문제 범위 안에서만!)\n        code_tables = problem_soup.find_all('table', class_='colorscripter-code-table')\n        \n        for table in code_tables:\n            # 테이블의 두 번째 td 찾기 (코드가 있는 칸)\n            rows = table.find_all('tr')\n            for row in rows:\n                tds = row.find_all('td')\n                if len(tds) >= 2:\n                    # 두 번째 td가 코드\n                    code_td = tds[1]\n                    \n                    # 모든 코드 라인 추출\n                    code_lines = []\n                    for div in code_td.find_all('div', style=lambda x: x and 'padding: 0 6px' in x):\n                        line = div.get_text()\n                        # HTML 엔티티 변환\n                        line = line.replace('&lt;', '<').replace('&gt;', '>')\n                        line = line.replace('&amp;', '&').replace('&nbsp;', ' ')\n                        line = line.replace('&quot;', '\"')\n                        code_lines.append(line)\n                    \n                    if code_lines:\n                        code = '\\n'.join(code_lines).strip()\n                        if len(code) > 20:  # 너무 짧은 건 제외\n                            code_blocks.append(code)\n        \n        return {\n            'full_text': problem_text,\n            'code_blocks': code_blocks\n        }\n    \n    def clean_html(self, text):\n        \"\"\"HTML 태그 제거\"\"\"\n        # <br>, <p> 등을 줄바꿈으로\n        text = re.sub(r'<br\\s*/?>', '\\n', text, flags=re.IGNORECASE)\n        text = re.sub(r'</p>', '\\n', text, flags=re.IGNORECASE)\n        \n        # 나머지 HTML 태그 제거\n        text = re.sub(r'<[^>]+>', '', text)\n        \n        # HTML 엔티티 변환\n        text = text.replace('&lt;', '<').replace('&gt;', '>')\n        text = text.replace('&amp;', '&').replace('&nbsp;', ' ')\n        text = text.replace('&quot;', '\"')\n        \n        # 연속된 공백/줄바꿈 정리\n        text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n        \n        return text.strip()\n    \n    def extract_code_problems(self, content):\n        \"\"\"개별 코드 문제 추출\"\"\"\n        text = content['text']\n        html = content['html']\n        year = content['year']\n        session = content['session']\n        url = content['url']\n        \n        problems = []\n        \n        # 언어별 패턴\n        language_patterns = {\n            'C': [\n                r'(\\d+)\\.\\s*다음은\\s*C\\s*언어.*?문제',\n                r'(\\d+)\\.\\s*다음은\\s*C언어.*?문제',\n            ],\n            'Java': [\n                r'(\\d+)\\.\\s*다음은\\s*Java.*?문제',\n                r'(\\d+)\\.\\s*다음은\\s*java.*?문제',\n                r'(\\d+)\\.\\s*다음은\\s*자바.*?문제',\n            ],\n            'Python': [\n                r'(\\d+)\\.\\s*다음은\\s*Python.*?문제',\n                r'(\\d+)\\.\\s*다음은\\s*python.*?문제',\n                r'(\\d+)\\.\\s*다음은\\s*파이썬.*?문제',\n                r'(\\d+)\\.\\s*다음은\\s*Pyhon.*?문제',\n            ]\n        }\n        \n        found_problems = set()  # 중복 방지\n        \n        for language, patterns in language_patterns.items():\n            for pattern in patterns:\n                matches = re.finditer(pattern, text, re.IGNORECASE)\n                for match in matches:\n                    problem_num = match.group(1)\n                    \n                    # 중복 체크\n                    key = f\"{year}-{session}-{problem_num}\"\n                    if key in found_problems:\n                        continue\n                    found_problems.add(key)\n                    \n                    # 문제 전체 내용 추출 (텍스트와 HTML 둘 다 전달)\n                    problem_content = self.extract_problem_content(text, html, problem_num)\n                    \n                    if problem_content:\n                        problems.append({\n                            'year': year,\n                            'session': session,\n                            'problem_num': problem_num,\n                            'language': language,\n                            'title': f\"[{year}년 {session}회] {problem_num}번 - {language}\",\n                            'url': url,\n                            'question': problem_content['full_text'],\n                            'code_blocks': problem_content['code_blocks']\n                        })\n        \n        return problems\n    \n    def crawl_all(self):\n        \"\"\"전체 크롤링 실행\"\"\"\n        post_links = self.get_post_links()\n        \n        if not post_links:\n            print(\"포스트를 찾을 수 없습니다.\")\n            return\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"크롤링 시작!\")\n        print(\"=\"*80)\n        \n        for idx, post_info in enumerate(post_links, 1):\n            print(f\"\\n[{idx}/{len(post_links)}] \", end=\"\")\n            \n            content = self.crawl_post_content(post_info)\n            \n            if content:\n                problems = self.extract_code_problems(content)\n                \n                if problems:\n                    self.code_problems.extend(problems)\n                    print(f\"  ✓ 코드 문제 {len(problems)}개 발견!\")\n                    for p in problems:\n                        print(f\"    - {p['problem_num']}번: {p['language']}\")\n                else:\n                    print(f\"  ⚠ 코드 문제 없음\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"크롤링 완료!\")\n        print(\"=\"*80)\n    \n    def get_statistics(self):\n        \"\"\"언어별 통계\"\"\"\n        c_count = sum(1 for p in self.code_problems if p['language'] == 'C')\n        java_count = sum(1 for p in self.code_problems if p['language'] == 'Java')\n        python_count = sum(1 for p in self.code_problems if p['language'] == 'Python')\n        \n        return {\n            'C': c_count,\n            'Java': java_count,\n            'Python': python_count,\n            'Total': len(self.code_problems)\n        }\n    \n    def save_to_json(self, filename='code_questions.json'):\n        \"\"\"JSON 저장\"\"\"\n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(self.code_problems, f, ensure_ascii=False, indent=2)\n            print(f\"\\n✓ JSON 파일 저장 완료: {filename}\")\n        except Exception as e:\n            print(f\"✗ JSON 저장 오류: {e}\")\n    \n    def save_to_txt(self, filename='code_questions.txt'):\n        \"\"\"TXT 저장\"\"\"\n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write(\"=\"*80 + \"\\n\")\n                f.write(\"정보처리기사 코드 문제 전체 목록\\n\")\n                f.write(\"=\"*80 + \"\\n\\n\")\n                \n                for p in self.code_problems:\n                    f.write(\"=\"*80 + \"\\n\")\n                    f.write(f\"제목: {p['title']}\\n\")\n                    f.write(f\"URL: {p['url']}\\n\")\n                    f.write(\"=\"*80 + \"\\n\\n\")\n                    \n                    f.write(\"[문제]\\n\")\n                    f.write(p['question'])\n                    f.write(\"\\n\\n\")\n                    \n                    if p['code_blocks']:\n                        f.write(\"[코드 블록]\\n\")\n                        for idx, code in enumerate(p['code_blocks'], 1):\n                            f.write(f\"\\n--- 코드 {idx} ---\\n\")\n                            f.write(code)\n                            f.write(\"\\n\" + \"-\"*40 + \"\\n\")\n                    \n                    f.write(\"\\n\\n\\n\")\n                \n                # 통계\n                stats = self.get_statistics()\n                f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n                f.write(\"📊 통계\\n\")\n                f.write(\"=\"*80 + \"\\n\")\n                f.write(f\"C언어: {stats['C']}개\\n\")\n                f.write(f\"Java: {stats['Java']}개\\n\")\n                f.write(f\"Python: {stats['Python']}개\\n\")\n                f.write(f\"\\n총 코드 문제: {stats['Total']}개\\n\")\n                f.write(\"=\"*80 + \"\\n\")\n            \n            print(f\"✓ TXT 파일 저장 완료: {filename}\")\n        except Exception as e:\n            print(f\"✗ TXT 저장 오류: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 크롤링 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"정보처리기사 코드 문제 크롤러 v3.4 - 진짜 최종!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "category_url = \"https://chobopark.tistory.com/category/Exam%20%26%20Study\"\n",
        "\n",
        "crawler = TistoryCrawler(category_url)\n",
        "\n",
        "# 크롤링\n",
        "crawler.crawl_all()\n",
        "\n",
        "# 통계\n",
        "stats = crawler.get_statistics()\n",
        "print(f\"\\n📊 최종 통계:\")\n",
        "print(f\"  C언어: {stats['C']}개\")\n",
        "print(f\"  Java: {stats['Java']}개\")\n",
        "print(f\"  Python: {stats['Python']}개\")\n",
        "print(f\"  \\n  🎯 총 코드 문제: {stats['Total']}개\\n\")\n",
        "\n",
        "# 저장\n",
        "crawler.save_to_json()\n",
        "crawler.save_to_txt()\n",
        "\n",
        "print(\"\\n완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 문제 샘플 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if crawler.code_problems:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"📝 문제 샘플 (첫 3개 문제)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, sample in enumerate(crawler.code_problems[:3], 1):\n",
        "        print(f\"\\n[{i}] {sample['title']}\")\n",
        "        print(f\"코드 블록: {len(sample['code_blocks'])}개\")\n",
        "        if sample['code_blocks']:\n",
        "            print(\"✅ 코드 있음!\")\n",
        "            print(f\"첫 줄: {sample['code_blocks'][0].split(chr(10))[0][:50]}...\")\n",
        "        else:\n",
        "            print(\"❌ 코드 없음\")\n",
        "else:\n",
        "    print(\"문제를 찾지 못했습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 파일 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files.download('code_questions.json')\n",
        "files.download('code_questions.txt')\n",
        "print(\"\\n파일 다운로드 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 상세 통계"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "by_year = {}\n",
        "for p in crawler.code_problems:\n",
        "    key = f\"{p['year']}년 {p['session']}회\"\n",
        "    if key not in by_year:\n",
        "        by_year[key] = []\n",
        "    by_year[key].append(p)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 년도별 상세 통계\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for year_session in sorted(by_year.keys(), reverse=True):\n",
        "    problems = by_year[year_session]\n",
        "    with_code = sum(1 for p in problems if p['code_blocks'])\n",
        "    print(f\"\\n[{year_session}]\")\n",
        "    print(f\"  총 문제: {len(problems)}개\")\n",
        "    print(f\"  코드 있음: {with_code}개 ✅\")\n",
        "    print(f\"  코드 없음: {len(problems) - with_code}개 ❌\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}