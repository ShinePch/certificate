{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì½”ë“œ ë¬¸ì œ í¬ë¡¤ëŸ¬ v3.4 (ì§„ì§œ ìµœì¢…!)\n",
        "\n",
        "**ğŸ¯ ë¬¸ì œ ë²”ìœ„ ì •í™•íˆ ì¶”ì¶œ!**\n",
        "\n",
        "## v3.4 ìˆ˜ì •\n",
        "- âœ… **ë¬¸ì œ ë²”ìœ„ë¥¼ ë¨¼ì € ì°¾ì€ í›„ ê·¸ ì•ˆì˜ ì½”ë“œë§Œ ì¶”ì¶œ**\n",
        "- âœ… ë‹¤ë¥¸ ë¬¸ì œì˜ ì½”ë“œê°€ ì„ì´ëŠ” ë²„ê·¸ ìˆ˜ì •\n",
        "- âœ… ê° ë¬¸ì œë§ˆë‹¤ ì •í™•í•œ ì½”ë“œ ë§¤ì¹­\n",
        "\n",
        "## ì‘ë™ ì›ë¦¬\n",
        "1. HTMLì—ì„œ \"12. ë‹¤ìŒì€\" ì°¾ê¸°\n",
        "2. \"13. ë‹¤ìŒì€\"ê¹Œì§€ì˜ ë²”ìœ„ ì¶”ì¶œ\n",
        "3. ê·¸ ë²”ìœ„ ì•ˆì˜ colorscripter-code-tableë§Œ íŒŒì‹±\n",
        "4. ì™„ë²½!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜ (ë¬¸ì œ ë²”ìœ„ ì •í™•íˆ!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TistoryCrawler:\n    def __init__(self, category_url):\n        self.category_url = category_url\n        self.base_url = \"https://chobopark.tistory.com\"\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        }\n        self.code_problems = []\n        \n    def get_post_links(self):\n        \"\"\"ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ëª¨ë“  í¬ìŠ¤íŠ¸ ë§í¬ ê°€ì ¸ì˜¤ê¸°\"\"\"\n        print(\"í¬ìŠ¤íŠ¸ ë§í¬ë¥¼ ìˆ˜ì§‘ ì¤‘...\")\n        \n        try:\n            response = requests.get(self.category_url, headers=self.headers)\n            response.encoding = 'utf-8'\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            post_links = []\n            \n            all_links = soup.find_all('a', href=True)\n            for link in all_links:\n                title = link.get_text(strip=True)\n                if 'ì •ë³´ì²˜ë¦¬ê¸°ì‚¬' in title and 'ì‹¤ê¸°' in title and 'ë³µì›' in title:\n                    full_url = urljoin(self.base_url, link['href'])\n                    if full_url not in [p['url'] for p in post_links]:\n                        year_match = re.search(r'(\\d{4})ë…„\\s*(\\d)íšŒ', title)\n                        if year_match:\n                            post_links.append({\n                                'url': full_url, \n                                'title': title,\n                                'year': year_match.group(1),\n                                'session': year_match.group(2)\n                            })\n            \n            post_links.sort(key=lambda x: (x['year'], x['session']), reverse=True)\n            \n            print(f\"ì´ {len(post_links)}ê°œì˜ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n            return post_links\n            \n        except Exception as e:\n            print(f\"ë§í¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}\")\n            return []\n    \n    def crawl_post_content(self, post_info):\n        \"\"\"ê°œë³„ í¬ìŠ¤íŠ¸ í¬ë¡¤ë§\"\"\"\n        print(f\"\\ní¬ë¡¤ë§ ì¤‘: [{post_info['year']}ë…„ {post_info['session']}íšŒ]\")\n        \n        try:\n            time.sleep(1)\n            response = requests.get(post_info['url'], headers=self.headers)\n            response.encoding = 'utf-8'\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            content_area = soup.find('div', class_='tt_article_useless_p_margin') or \\\n                          soup.find('article') or \\\n                          soup.find('div', {'class': 'content'})\n            \n            if not content_area:\n                content_area = soup.find('div', id='content') or soup.find('main')\n            \n            if content_area:\n                return {\n                    'text': content_area.get_text(separator='\\n', strip=True),\n                    'html': str(content_area),\n                    'year': post_info['year'],\n                    'session': post_info['session'],\n                    'url': post_info['url']\n                }\n            else:\n                print(f\"  âœ— ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n                return None\n                \n        except Exception as e:\n            print(f\"  âœ— í¬ë¡¤ë§ ì˜¤ë¥˜: {e}\")\n            return None\n    \n    def extract_problem_content(self, text, html, problem_num):\n        \"\"\"íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ì˜ ì „ì²´ ë‚´ìš© ì¶”ì¶œ\"\"\"\n        from bs4 import BeautifulSoup\n        \n        # HTML íŒŒì‹±\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        # ì „ì²´ HTMLì„ ë¬¸ìì—´ë¡œ\n        html_str = str(soup)\n        \n        # ë¬¸ì œ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (HTMLì—ì„œ)\n        problem_pattern = rf'{problem_num}\\.\\s*ë‹¤ìŒì€'\n        problem_match = re.search(problem_pattern, html_str)\n        \n        if not problem_match:\n            return None\n        \n        problem_start_pos = problem_match.start()\n        \n        # ë‹¤ìŒ ë¬¸ì œ ì°¾ê¸°\n        next_num = int(problem_num) + 1\n        next_pattern = rf'{next_num}\\.\\s*ë‹¤ìŒì€'\n        next_match = re.search(next_pattern, html_str[problem_start_pos + 10:])\n        \n        if next_match:\n            problem_end_pos = problem_start_pos + 10 + next_match.start()\n        else:\n            problem_end_pos = len(html_str)\n        \n        # ë¬¸ì œ ë²”ìœ„ì˜ HTML ì¶”ì¶œ\n        problem_html = html_str[problem_start_pos:problem_end_pos]\n        \n        # ì´ ë²”ìœ„ì˜ HTMLì„ ë‹¤ì‹œ íŒŒì‹±\n        problem_soup = BeautifulSoup(problem_html, 'html.parser')\n        \n        # ë¬¸ì œ í…ìŠ¤íŠ¸ (ì •ë¦¬ëœ ë²„ì „)\n        problem_text = problem_soup.get_text().strip()\n        \n        # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ - Color Scripter í…Œì´ë¸”ë§Œ ì°¾ê¸°\n        code_blocks = []\n        \n        # colorscripter-code-table ì°¾ê¸° (ì´ì œ ë¬¸ì œ ë²”ìœ„ ì•ˆì—ì„œë§Œ!)\n        code_tables = problem_soup.find_all('table', class_='colorscripter-code-table')\n        \n        for table in code_tables:\n            # í…Œì´ë¸”ì˜ ë‘ ë²ˆì§¸ td ì°¾ê¸° (ì½”ë“œê°€ ìˆëŠ” ì¹¸)\n            rows = table.find_all('tr')\n            for row in rows:\n                tds = row.find_all('td')\n                if len(tds) >= 2:\n                    # ë‘ ë²ˆì§¸ tdê°€ ì½”ë“œ\n                    code_td = tds[1]\n                    \n                    # ëª¨ë“  ì½”ë“œ ë¼ì¸ ì¶”ì¶œ\n                    code_lines = []\n                    for div in code_td.find_all('div', style=lambda x: x and 'padding: 0 6px' in x):\n                        line = div.get_text()\n                        # HTML ì—”í‹°í‹° ë³€í™˜\n                        line = line.replace('&lt;', '<').replace('&gt;', '>')\n                        line = line.replace('&amp;', '&').replace('&nbsp;', ' ')\n                        line = line.replace('&quot;', '\"')\n                        code_lines.append(line)\n                    \n                    if code_lines:\n                        code = '\\n'.join(code_lines).strip()\n                        if len(code) > 20:  # ë„ˆë¬´ ì§§ì€ ê±´ ì œì™¸\n                            code_blocks.append(code)\n        \n        return {\n            'full_text': problem_text,\n            'code_blocks': code_blocks\n        }\n    \n    def clean_html(self, text):\n        \"\"\"HTML íƒœê·¸ ì œê±°\"\"\"\n        # <br>, <p> ë“±ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ\n        text = re.sub(r'<br\\s*/?>', '\\n', text, flags=re.IGNORECASE)\n        text = re.sub(r'</p>', '\\n', text, flags=re.IGNORECASE)\n        \n        # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ì œê±°\n        text = re.sub(r'<[^>]+>', '', text)\n        \n        # HTML ì—”í‹°í‹° ë³€í™˜\n        text = text.replace('&lt;', '<').replace('&gt;', '>')\n        text = text.replace('&amp;', '&').replace('&nbsp;', ' ')\n        text = text.replace('&quot;', '\"')\n        \n        # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬\n        text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n        \n        return text.strip()\n    \n    def extract_code_problems(self, content):\n        \"\"\"ê°œë³„ ì½”ë“œ ë¬¸ì œ ì¶”ì¶œ\"\"\"\n        text = content['text']\n        html = content['html']\n        year = content['year']\n        session = content['session']\n        url = content['url']\n        \n        problems = []\n        \n        # ì–¸ì–´ë³„ íŒ¨í„´\n        language_patterns = {\n            'C': [\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*C\\s*ì–¸ì–´.*?ë¬¸ì œ',\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*Cì–¸ì–´.*?ë¬¸ì œ',\n            ],\n            'Java': [\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*Java.*?ë¬¸ì œ',\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*java.*?ë¬¸ì œ',\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*ìë°”.*?ë¬¸ì œ',\n            ],\n            'Python': [\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*Python.*?ë¬¸ì œ',\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*python.*?ë¬¸ì œ',\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*íŒŒì´ì¬.*?ë¬¸ì œ',\n                r'(\\d+)\\.\\s*ë‹¤ìŒì€\\s*Pyhon.*?ë¬¸ì œ',\n            ]\n        }\n        \n        found_problems = set()  # ì¤‘ë³µ ë°©ì§€\n        \n        for language, patterns in language_patterns.items():\n            for pattern in patterns:\n                matches = re.finditer(pattern, text, re.IGNORECASE)\n                for match in matches:\n                    problem_num = match.group(1)\n                    \n                    # ì¤‘ë³µ ì²´í¬\n                    key = f\"{year}-{session}-{problem_num}\"\n                    if key in found_problems:\n                        continue\n                    found_problems.add(key)\n                    \n                    # ë¬¸ì œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ì™€ HTML ë‘˜ ë‹¤ ì „ë‹¬)\n                    problem_content = self.extract_problem_content(text, html, problem_num)\n                    \n                    if problem_content:\n                        problems.append({\n                            'year': year,\n                            'session': session,\n                            'problem_num': problem_num,\n                            'language': language,\n                            'title': f\"[{year}ë…„ {session}íšŒ] {problem_num}ë²ˆ - {language}\",\n                            'url': url,\n                            'question': problem_content['full_text'],\n                            'code_blocks': problem_content['code_blocks']\n                        })\n        \n        return problems\n    \n    def crawl_all(self):\n        \"\"\"ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰\"\"\"\n        post_links = self.get_post_links()\n        \n        if not post_links:\n            print(\"í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n            return\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"í¬ë¡¤ë§ ì‹œì‘!\")\n        print(\"=\"*80)\n        \n        for idx, post_info in enumerate(post_links, 1):\n            print(f\"\\n[{idx}/{len(post_links)}] \", end=\"\")\n            \n            content = self.crawl_post_content(post_info)\n            \n            if content:\n                problems = self.extract_code_problems(content)\n                \n                if problems:\n                    self.code_problems.extend(problems)\n                    print(f\"  âœ“ ì½”ë“œ ë¬¸ì œ {len(problems)}ê°œ ë°œê²¬!\")\n                    for p in problems:\n                        print(f\"    - {p['problem_num']}ë²ˆ: {p['language']}\")\n                else:\n                    print(f\"  âš  ì½”ë“œ ë¬¸ì œ ì—†ìŒ\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"í¬ë¡¤ë§ ì™„ë£Œ!\")\n        print(\"=\"*80)\n    \n    def get_statistics(self):\n        \"\"\"ì–¸ì–´ë³„ í†µê³„\"\"\"\n        c_count = sum(1 for p in self.code_problems if p['language'] == 'C')\n        java_count = sum(1 for p in self.code_problems if p['language'] == 'Java')\n        python_count = sum(1 for p in self.code_problems if p['language'] == 'Python')\n        \n        return {\n            'C': c_count,\n            'Java': java_count,\n            'Python': python_count,\n            'Total': len(self.code_problems)\n        }\n    \n    def save_to_json(self, filename='code_questions.json'):\n        \"\"\"JSON ì €ì¥\"\"\"\n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(self.code_problems, f, ensure_ascii=False, indent=2)\n            print(f\"\\nâœ“ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n        except Exception as e:\n            print(f\"âœ— JSON ì €ì¥ ì˜¤ë¥˜: {e}\")\n    \n    def save_to_txt(self, filename='code_questions.txt'):\n        \"\"\"TXT ì €ì¥\"\"\"\n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write(\"=\"*80 + \"\\n\")\n                f.write(\"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì½”ë“œ ë¬¸ì œ ì „ì²´ ëª©ë¡\\n\")\n                f.write(\"=\"*80 + \"\\n\\n\")\n                \n                for p in self.code_problems:\n                    f.write(\"=\"*80 + \"\\n\")\n                    f.write(f\"ì œëª©: {p['title']}\\n\")\n                    f.write(f\"URL: {p['url']}\\n\")\n                    f.write(\"=\"*80 + \"\\n\\n\")\n                    \n                    f.write(\"[ë¬¸ì œ]\\n\")\n                    f.write(p['question'])\n                    f.write(\"\\n\\n\")\n                    \n                    if p['code_blocks']:\n                        f.write(\"[ì½”ë“œ ë¸”ë¡]\\n\")\n                        for idx, code in enumerate(p['code_blocks'], 1):\n                            f.write(f\"\\n--- ì½”ë“œ {idx} ---\\n\")\n                            f.write(code)\n                            f.write(\"\\n\" + \"-\"*40 + \"\\n\")\n                    \n                    f.write(\"\\n\\n\\n\")\n                \n                # í†µê³„\n                stats = self.get_statistics()\n                f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n                f.write(\"ğŸ“Š í†µê³„\\n\")\n                f.write(\"=\"*80 + \"\\n\")\n                f.write(f\"Cì–¸ì–´: {stats['C']}ê°œ\\n\")\n                f.write(f\"Java: {stats['Java']}ê°œ\\n\")\n                f.write(f\"Python: {stats['Python']}ê°œ\\n\")\n                f.write(f\"\\nì´ ì½”ë“œ ë¬¸ì œ: {stats['Total']}ê°œ\\n\")\n                f.write(\"=\"*80 + \"\\n\")\n            \n            print(f\"âœ“ TXT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}\")\n        except Exception as e:\n            print(f\"âœ— TXT ì €ì¥ ì˜¤ë¥˜: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. í¬ë¡¤ë§ ì‹¤í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì½”ë“œ ë¬¸ì œ í¬ë¡¤ëŸ¬ v3.4 - ì§„ì§œ ìµœì¢…!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "category_url = \"https://chobopark.tistory.com/category/Exam%20%26%20Study\"\n",
        "\n",
        "crawler = TistoryCrawler(category_url)\n",
        "\n",
        "# í¬ë¡¤ë§\n",
        "crawler.crawl_all()\n",
        "\n",
        "# í†µê³„\n",
        "stats = crawler.get_statistics()\n",
        "print(f\"\\nğŸ“Š ìµœì¢… í†µê³„:\")\n",
        "print(f\"  Cì–¸ì–´: {stats['C']}ê°œ\")\n",
        "print(f\"  Java: {stats['Java']}ê°œ\")\n",
        "print(f\"  Python: {stats['Python']}ê°œ\")\n",
        "print(f\"  \\n  ğŸ¯ ì´ ì½”ë“œ ë¬¸ì œ: {stats['Total']}ê°œ\\n\")\n",
        "\n",
        "# ì €ì¥\n",
        "crawler.save_to_json()\n",
        "crawler.save_to_txt()\n",
        "\n",
        "print(\"\\nì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ë¬¸ì œ ìƒ˜í”Œ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if crawler.code_problems:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“ ë¬¸ì œ ìƒ˜í”Œ (ì²« 3ê°œ ë¬¸ì œ)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i, sample in enumerate(crawler.code_problems[:3], 1):\n",
        "        print(f\"\\n[{i}] {sample['title']}\")\n",
        "        print(f\"ì½”ë“œ ë¸”ë¡: {len(sample['code_blocks'])}ê°œ\")\n",
        "        if sample['code_blocks']:\n",
        "            print(\"âœ… ì½”ë“œ ìˆìŒ!\")\n",
        "            print(f\"ì²« ì¤„: {sample['code_blocks'][0].split(chr(10))[0][:50]}...\")\n",
        "        else:\n",
        "            print(\"âŒ ì½”ë“œ ì—†ìŒ\")\n",
        "else:\n",
        "    print(\"ë¬¸ì œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. íŒŒì¼ ë‹¤ìš´ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files.download('code_questions.json')\n",
        "files.download('code_questions.txt')\n",
        "print(\"\\níŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ìƒì„¸ í†µê³„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "by_year = {}\n",
        "for p in crawler.code_problems:\n",
        "    key = f\"{p['year']}ë…„ {p['session']}íšŒ\"\n",
        "    if key not in by_year:\n",
        "        by_year[key] = []\n",
        "    by_year[key].append(p)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š ë…„ë„ë³„ ìƒì„¸ í†µê³„\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for year_session in sorted(by_year.keys(), reverse=True):\n",
        "    problems = by_year[year_session]\n",
        "    with_code = sum(1 for p in problems if p['code_blocks'])\n",
        "    print(f\"\\n[{year_session}]\")\n",
        "    print(f\"  ì´ ë¬¸ì œ: {len(problems)}ê°œ\")\n",
        "    print(f\"  ì½”ë“œ ìˆìŒ: {with_code}ê°œ âœ…\")\n",
        "    print(f\"  ì½”ë“œ ì—†ìŒ: {len(problems) - with_code}ê°œ âŒ\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}